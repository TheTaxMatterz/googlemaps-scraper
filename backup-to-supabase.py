# This file does the following:
# 1. Reads the csv file generated by python scraper.py --N 200 --i urls2.txt as a pandas object.
# 2. Converts the pandas object to json.
# 3. Uploads the data Supabase so that it can be read later on.
import os
import json
import dotenv
import pandas as pd
import json
from supabase import create_client, Client

# 0.0 Pseudo Comments
# 0.1 loads the dotenv file or environment variables from within bash/terminal
dotenv.load_dotenv()

# 2.0 get the key and url ready to connect to supabase
# 2.0 os independent way of getting and setting sensitive values works on windows and ubuntu
url = os.getenv('SUPABASE_URL')
key = os.getenv('SUPABASE_KEY')
# 2.0.1 for testing purposes only
# print(url, key)

# 2.1 Connect to the Supabase Database and create a supabase python connection reference
supabase: Client = create_client(url, key)

# 2.1 Insert a new row in ReviewDateID table
# - This works because both columns in ReviewDate are auto generated by the Postgres Database Engine in the backend
returnvalue = supabase.table("ReviewDate").insert([
    {}
]).execute()

# 2.2 from the above insert get the reviewDateID as a foreign key for the insert of the data
# - how to read the returned value (especially a particular column)
# - return value below:
# - data=[{'reviewDate': '2022-02-13T08:42:59.060013', 'reviewDateID': 32}] count=None
reviewDateID = returnvalue.data[0]['reviewDateID']
# reviewDateID = 43
# 2.2.1 for testing purposes only
# - print the returned reviewdateid from the insert above
print(reviewDateID)

# 3.0 These steps have to be done later because we need to add a new column from the returnvalue
# - as the default value
# 1.0 Read the data generated by the scraper.py file
# 1.1 Read only the required columns (in the above task) and store as a pandas dataframe df
# 1.1.1 Define only the required columns/fields
columns = [
    "caption",
    "rating",
    "username"
]
# df = pd.read_csv(r'./data/newest_gm_reviews.csv', usecols=columns)
df = pd.read_csv(r'./data/newest_gm_reviews.csv')
# 1.1.2 Filter all reviews where the rating is 5 only
# df = df[df['rating'] == 5]
# 1.2 convert the data frame to json so that it can be uploaded to supabase

# 1.2.1 Just before we convert to json we add a new field reviewDateID as the foreign key
df['reviewDateID'] = reviewDateID

# Write a JSON File
data = df.to_json(r'./data/newest_gm_reviews.json', orient='records')
# 1.3.1 this reads the previously saved json file and converts that file to json
json_data = pd.read_json(r'./data/newest_gm_reviews.json').to_json(orient='records')
# For testing purposes
print(json_data)

# 2.3 Insert the json data from previous steps to supabase
returnvalue = supabase.table("Review").insert(json.loads(json_data)).execute()
